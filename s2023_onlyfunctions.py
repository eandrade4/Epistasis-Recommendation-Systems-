# -*- coding: utf-8 -*-
"""S2023 OnlyFunctions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RsBTlLBLfLuYNOiPJ8z9NqikpclEHT3i

This file contains all of the imports and functions used within the clustering and NERDS algorithm.

#Import and Preprocessing
"""

#pip install kneed

#pip install kaleido

import tarfile
import urllib
import time
from datetime import date
import os
from datetime import datetime


import kaleido
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from scipy import stats
from itertools import combinations
import math
import collections
from matplotlib_venn import venn2

from kneed import KneeLocator, DataGenerator as dg
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler

# from google.colab import drive
# drive.mount('/content/drive')

def drive_setup(str_output_folder_path):
  '''As an input, this function takes a string that is the file path to a folder in your drive. 
  The output to the code will be placed in this folder. 
  
  Make sure google drive is mounted. You will be asked to approve this. 
  A new directory is created with the current date and time as the name, and returned. '''
  #Mount Google Drive so you can import desired spreadsheet and create a folder for each thing we output. 

  now = datetime.now()
  images_dir = f"{str_output_folder_path}/{now}"
  os.mkdir(images_dir)

  return images_dir

def standardize_features(dfphen,phenotype): 
  '''This function takes a dataframe containing the phenotypes and genotypes of individuals and the phenotype, which is an output of import_clean() as inputs. 
  The genotypes are normalized, and put into a numpy array. 
  
  This function returns a dataframe and 2 numpy arrays: 
  df: the dataframe consisting of just the genotypes. 
  features: the numpy array version of df.
  scaled_features: the standardized version of features. 
    '''

  df = dfphen.sort_index(axis=1).drop(phenotype, axis = 1)
  #We will compare the genotypes, so sklearn needs a numpy array with only the genotypes. 
  #We will later cluster these genotypes using a clustering pipeline. 
  features = df.to_numpy()
  #normalize the data
  scaler = StandardScaler()
  scaled_features = scaler.fit_transform(features)

  return df, features, scaled_features

def import_clean(phen, cols, path):
  '''This function reads a file containing a table, turns it into a data frame, and sets up the df in the format we need for the rest of the algorithm. 

        Prior to importing your csv or excel file, make sure that the phenotype column comes before the genotype columns. 
        This will ensure that later, when you list the column numbers it is in increasing order. 
        Import a csv or excel file using the local computer or Google Drive. 
        If you use Google drive for the path, make sure to import and mount Drive. 
        The dataframe will consist of rows containing the phenotype of an individual, and the genotype. 
        The entries of the columns for each SNP will consist of 0, 1, or 2 depending on how many copies of each mutation are present. 
        Any individual with an incomplete genotype will not be considered. 
        The indices are reset. The phenotype string name is returned and called phenotype. 
        Additionally, a dataframe is created and returned containing the genotype and phenotype, and is called "dfphen". 
    

        Parameters
        ----------
        phen: This is the name of the column that contains the phenotypes of the individuals. 
        cols: A list of the columns of the file that you wish to be included in the dataframe (phenotype and genotype columns).
        path: The path to the file location. This can be on the local computer or Google Drive. 
  '''

  phenotype = phen

  try:
    dfphen = pd.read_csv(path,  na_values='-', 
                 usecols=cols)
  except:
    dfphen = pd.read_excel(path,  na_values='-', 
                 usecols=cols)
  
  dfphen.columns = np.append(phenotype, list(range(0,len(dfphen.axes[1])-1)))
  dfphen = dfphen.dropna()
  dfphen.reset_index(drop=True, inplace=True)
  
  return phenotype, dfphen

def elbow_method(df, scaled_features, images_dir): 
  '''This function runs a k-means clustering optimization scheme by using the sklearn k-means library. 
  We use do 10 random initializations with 300 as the max iterations, and the random state of 42. 

  ---Parameters---
  df: the dataframe consisting of just the genotypes. Returned by standardize_features(). 
  scaled_features: the standardized version of df as a numpy array. Returned by standardize_features(). 
  images_dir: the path to the directory where your visualization will be saved. This is returned by drive_setup(). 

  
  Within this function, a curve is created for the relationship between values for k (the nummber of clusters) and the sum of squares error. 
  Then, the elbow of this curve is found using the kneed package, which you may need to pip install. 
  Once this is done, the function returns the number of clusters at the elbow, and saves the graph to the directory previously specified in the drive_setup function. '''
  kmeans_kwargs = {
    "init": "random",
    "n_init": 10,
    "max_iter": 300,
    "random_state": 42,
  } 

  # A list holds the SSE values for each k
  sse = []
  for k in range(1, len(df.columns) + 1):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(scaled_features)
    sse.append(kmeans.inertia_)

  plt.style.use("fivethirtyeight")
  plt.plot(range(1, len(df.columns) + 1), sse)
  plt.xticks(range(1, len(df.columns) + 1))
  plt.xlabel("Number of Clusters")
  plt.ylabel("SSE")
   
  plt.savefig(f"{images_dir}/elbow.png",bbox_inches='tight')
  plt.show()

 #Thereâ€™s a sweet spot where the SSE curve starts to bend known as the elbow point. 
 #The x-value of this point is thought to be a reasonable trade-off between error and number of clusters.

  kl = KneeLocator(
    range(1, len(df.columns) + 1), sse, curve="convex", direction="decreasing"
  )

  kl.elbow
  
  #kl.plot_knee()

  return kl.elbow

def sil_method(df, scaled_features, images_dir): 
  '''This functions determines the relationship between the number of clusters and a metric called the silhouette coefficient. 
  This coefficient ranges from 0 to 1 and measures how well the data points fit in their given clusters based on intracluster distance and intercluster distance. 
  
  ---Parameters---
  df: the dataframe consisting of just the genotypes. Returned by standardize_features(). 
  scaled_features: the standardized version of df as a numpy array. Returned by standardize_features(). 
  images_dir: the path to the directory where your visualization will be saved. This is returned by drive_setup(). 

  

  A visual is created to demonstrate how the number of clusters affects the Silhouette score. 
  The number of clusters that corresponds with the highest score is returned by the function. '''
  # A list holds the silhouette coefficients for each k
  silhouette_coefficients = []

  kmeans_kwargs = {
    "init": "random",
    "n_init": 10,
    "max_iter": 300,
    "random_state": 42,
  } 

  # Notice you start at 2 clusters for silhouette coefficient
  for k in range(2, len(df.columns) + 1):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(scaled_features)
    score = silhouette_score(scaled_features, kmeans.labels_)
    silhouette_coefficients.append(score)

  plt.style.use("fivethirtyeight")
  plt.plot(range(2, len(df.columns) + 1), silhouette_coefficients)
  plt.xticks(range(2, len(df.columns) + 1))
  plt.xlabel("Number of Clusters")
  plt.ylabel("Silhouette Coefficient")
  plt.savefig(f"{images_dir}/silhouette.png",bbox_inches='tight')
  plt.show()

  #we add two, since we started with 2 clusters, and since it is 0 indexed. 
  silhouette_coefficients_max = silhouette_coefficients.index(max(silhouette_coefficients)) + 2
  silhouette_coefficients_max
  return silhouette_coefficients_max

def choose_num_clusters(df, scaled_features, images_dir): 
  '''This function determines the number of clusters that is most optimal for our data using two methods: the elbow method and the silhouette coefficient method. 
  
  The minimum output of these two methodologies is the number of clusters we will proceed with. 
  This is thought to be an optimal balance between error and computational demand. 
  '''
  return min(elbow_method(df, scaled_features, images_dir),sil_method(df, scaled_features, images_dir))

def cluster_features(k,scaled_features): 
  '''The cluster_features function takes two inputs, k and the scaled_features. 
  Note that k is the output of choose_num_clusters() and scaled_features is a global variable created within the function standardize_features(), which is called by import_clean(). 
  
  ---Parameters---
  df: the dataframe consisting of just the genotypes. Returned by standardize_features(). 
  scaled_features: the standardized version of df as a numpy array. Returned by standardize_features(). 
  images_dir: the path to the directory where your visualization will be saved. This is returned by drive_setup(). 


  This function runs a k-means clustering algorithm with k many clusters. 
  It also fits the features so we can see what the assignments are using kmeans.labels_[:] if desired. 
  A  variable called kmeans is created with these properties and is returned by this function.'''
  
  kmeans = KMeans(
    init="random",
    n_clusters=k,
    n_init=50,
    max_iter=500,
    random_state=42
  )

  kmeans.fit(scaled_features)
  
  return kmeans

def cluster_viz(k, title_of_plot, features, images_dir): 
  '''This function reduces our data into 2 components for clustering so we can see them visually. 
  The input to this function is k (number of clusters). 
  This function saves the output to the image directory specified in drive_setup(). 

  ---Parameters---
  k: the number of clusters. Returned by choose_num_clusters(df, scaled_features, images_dir). 
  features: the df containing the genotypes as a numpy array. Returned by standardize_features(). 
  images_dir: the path to the directory where your visualization will be saved. This is returned by drive_setup(). 

  We use a preprocessing pipeline to scale the data using MinMaxScaler(), then reduce to 2 components using PCA and random state 42. 
  Next, a clustering pipeline is created, the features are refit, and a visualization is created. '''

  #dimensionality reduction
  #PCA down to 2 components, defined by n_components = 2
  #minmaxscaler is used instead of standard scalar (standardization shifts so each feature has a mean of 0 and stdev of 1.)
  #This is because we DO NOT ASSUME that the shape of all our features (presence of snps) follows a normal disribution. 
  #feature scaling is important because it levels the playing field for the features, and influences the performance of the algorithm. 
  preprocessor = Pipeline(
    [
        ("scaler", MinMaxScaler()),
        ("pca", PCA(n_components=2, random_state=42)),
    ]
  )

  #build a separate pipeline for k means clustering 
  clusterer = Pipeline(
    [
        (
            "kmeans",
            KMeans(
                n_clusters=k,
                init="k-means++", #instead of the default "random", this ensures that there is distance between the initialized centroids. 
                n_init=50, #the number of times the clustering method is ran with new initialized centroids - to find a stable soln. 
                max_iter=500, # this is the max # of times out centroids are moved to the center of the new cluster - high to ensure convergence. 
                random_state=42,
            ),
        ),
    ]
  ) 

  #combines the two pipelines we made. 
  pipe = Pipeline([("preprocessor", preprocessor), ("clusterer", clusterer)])
  
  #This performs all the steps of the pipeline on the data. preprocessing then clustering 
  pipe.fit(features)

  #calculates the silhouette coefficient 
  #calculates how well the data points fit into their assigned cluster based on
  #intra-cluster distance - how far the points are from other points in the cluster 
  #inter-culster distance - how far the points are from points in other clusters.
  #b = mean nearest cluster distance for each sample 
  #a = mean intra-cluster distance
  #silhouette score = (b-a)/max(b,a)
  #between -1 and 1. values near 0 indicate overlapping clusters. 
  preprocessed_data = pipe["preprocessor"].transform(features)
  predicted_labels = pipe["clusterer"]["kmeans"].labels_
  silhouette_score(preprocessed_data, predicted_labels)

  pcadf = pd.DataFrame(
    pipe["preprocessor"].transform(features),
    columns=["component_1", "component_2"],
  )

  pcadf["predicted_cluster"] = pipe["clusterer"]["kmeans"].labels_

  plt.style.use("fivethirtyeight")
  plt.figure(figsize=(8, 8))

  scat = sns.scatterplot(
    x = "component_1",
    y = "component_2",
    s=50,
    data=pcadf,
    hue="predicted_cluster",
    palette = 'flare' #'Paired' is also a good palette. I like this one. for the default just take out the palette. 
  )

  scat.set_title(title_of_plot)
  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)
  plt.savefig(f"{images_dir}/clusters2d.png",bbox_inches='tight')
  plt.show()

def df_clusters(k,dfphen,kmeans):
  '''This function takes k (the number of clusters), dfphen (the dataframe created by the function import_clean() as inputs), and kmeans (the clustering pipeline returned by cluster_features(k,scaled_features)) as parameters. 
  A  variable named "clusters" is created and returned. This contains a list of k many dataframes that consist of the individuals in each cluster. 
  To access the dataframes for cluster i, where 0 <= i <= k-1, call clusters[i]. '''

  clusters = []
  for i in range(k): 
    clusters.append("cluster" + str(i) + "df")
  
  for i in range(k): 
    clusters[i] = dfphen[kmeans.labels_ == i]

  return clusters

def choose_finaldf(str_max_or_min, clusters, phenotype):
  '''The choose_finaldf function determines the cluster with the highest or lowest average phenotype.

  ---Parameters--- 
  str_max_or_min: If you will analyze a data set in terms of extreme phenotypes in the positive direction, the parameter str_max_or_min should be "max". 
  Otherwise, write "min". Note that the default is "max" if your string does not match either value. 
  
  clusters: the individuals within their cluster assignments. Returned by df_clusters(k,dfphen,kmeans). 
  phenotype: returned by import_clean(phen, cols, path).
  
  The chosen dataframe is saved as a variable called finaldf, and returned. 
  Additionally, the index of the final cluster in the list "clusters" is recorded under the variable n and returned. '''
  means = []

  if str_max_or_min == "min":
    for df in clusters:
      means.append(df[phenotype].mean())

    means

    n = means.index(min(means))
    finaldf = clusters[n]
  else:
    for df in clusters:
      means.append(df[phenotype].mean())

    means

    n = means.index(max(means))
    finaldf = clusters[n]

    return finaldf, n

def max_data_fun(df,index, phenotype):
  '''This function sums the genotypes within the data frame of the indices the user provides and outputs their total occurrences.
  The phenotype parameter was returned by import_clean(phen, cols, path). 
  The goal is to determine the snps that occur the most often within the list of similar individuals. '''
  max_dt= df.loc[index]
  max_dt = max_dt.drop(columns=[phenotype])
  high= max_dt.sum(axis=0)
  return high

def occurrences_viz(finaldf, images_dir, phenotype, n): 
  '''To ensure this image saves to the directory specified, make sure to pip install kaleido.

  ---Parameters---
  finaldf: The df that you are analzing the occurrence of snps in. Returned by choose_finaldf(str_max_or_min, clusters, phenotype). 
  images_dir: The path to the directory where your visualization will be saved. This is returned by drive_setup(). 
  phenotype: The string of the phenotype name, which is returned by import_clean(phen, cols, path).
  n: the number of the cluster that was chosen and returned by choose_finaldf(str_max_or_min, clusters, phenotype).

  This is the graph of the occurrences of the first order mutations in the finaldf dataframe.''' 
  #graph of the first order strengths
  fig = px.bar(max_data_fun(finaldf, finaldf.index, phenotype), labels={'x':'SNP', 'y':'occurrences'})
  fig.update_layout(title=f'SNP Occurrences in Cluster {n}', xaxis_title="SNP", yaxis_title="Occurrences")
  fig.write_image(f"{images_dir}/occurrences.png", engine="kaleido",width=1000, height=500)
  fig.show()

def clustervsavg_viz(dfphen, finaldf, phenotype, images_dir,n):
  '''This function creates a scatterplot of the phenotype for each of the individuals in the study. 

  ---Parameters---
  dfphen: The dataframe containing all the individuals and the phenotype. Returned by import_clean(phen, cols, path). 
  finaldf: The df that you are analzing the occurrence of snps in. Returned by choose_finaldf(str_max_or_min, clusters, phenotype). 
  phenotype: The phenotype string. Returned by import_clean(phen, cols, path).
  images_dir: the path to the directory where your visualization will be saved. This is returned by drive_setup(). 
  n: the number of the cluster that was chosen and returned by choose_finaldf(str_max_or_min, clusters, phenotype).

  The average phenotype of individuals in the specified cluster from finaldf is compared to the overall average phenotype of individuals in dfphen. '''
  # create empty figure
  fig = go.Figure()

  # add a scatterplot (Individuals vs Phenotype)
  fig.add_trace(go.Scatter(x=dfphen.index, y=dfphen.loc[:,phenotype], mode='markers',name ='Individuals',  marker=dict(
            color='steelblue')))
  fig.add_trace(go.Scatter(x=finaldf.index, y=finaldf.loc[:,phenotype], mode='markers', name=f'Individuals in Cluster {n}', marker=dict(
            color='coral')))
  fig.add_trace(go.Scatter(x = [dfphen.index.min(), dfphen.index.max()],
                           y = [dfphen[phenotype].mean(), dfphen[phenotype].mean()],
                           mode = "lines", name='Average of all Individuals', line=dict(color='steelblue')))
  fig.add_trace(go.Scatter(x = [dfphen.index.min(), dfphen.index.max()],
                           y = [finaldf.loc[:,phenotype].mean(), finaldf.loc[:,phenotype].mean()],
                           mode = "lines", name=f'Average of Individuals in Cluster {n}', line=dict(color='coral')))

  # add labels and title
  fig.update_layout(title=f'Cluster {n} Individuals vs Average - Nonbinary', xaxis_title="Individuals", yaxis_title=f"Phenotype ({phenotype})")
  fig.write_image(f"{images_dir}/clustervsavg.png", engine="kaleido",width=1000, height=500)
  fig.show()

def firstorder(snp1, dfphen, phenotype):
  '''This function performs a t-test to determine if the first order snp coalition [snp1] is statistically significant using dfphen. 

  Other parameters include: 
  dfphen: The dataframe containing all the individuals and the phenotype. Returned by import_clean(phen, cols, path). 
  phenotype: The phenotype string. Returned by import_clean(phen, cols, path).

  In other words, we determine if there is sufficient evidence that [snp1] influences the phenotype. The p-value is returned. '''
  #Group where snp1 is on
  true = dfphen.loc[(dfphen[snp1] >= 1), phenotype]

  #Group where snp1 is off
  false = dfphen.loc[(dfphen[snp1] == 0), phenotype]
    
  #p-value
  return stats.ttest_ind(true,false).pvalue

def order1groupings(originaldf, specifieddf, amount, dfphen, phenotype):
  '''As inputs, this function takes the original dataframe of the study containing all of the individuals, the dataframe containing the specified individuals to consider, the amount of first order SNP coalitions to be recommended, dfphen (returned by import_clean(phen, cols, path)), and the phenotype string. 
  
  Within this function, the total occurrences of each snp in the specifieddf is counted, and an adjusted count is calculated. 
  The adjusted count takes the occurrences of each first order snp grouping in specifieddf, and divides by the occurrences in the originaldf. 
  The SNP coalitions corresponding to the top adjusted counts are tested using a t-test and a dataframe called o1recs containing the recommended SNP groupings, the count, the adjusted count, and their p-values is returned. 

  Additionally, o1recs, which is the order 1 recommendations is returned. 
  '''
  snps = max_data_fun(originaldf,specifieddf.index,phenotype).index
  pvals=[]
  t=len(snps)
  for i in range(t):
    pvals.append(firstorder(snps[i],dfphen, phenotype))
  o1recs = pd.DataFrame()
  o1recs['snps'] = snps
  o1recs['count'] = (max_data_fun(originaldf,specifieddf.index,phenotype).values)
  #adjusted count divides by the total number of individuals in df2 that have that group of snps. 
  o1recs['adjcount'] = (max_data_fun(originaldf,specifieddf.index,phenotype).values)/(max_data_fun(originaldf,originaldf.index,phenotype).values)
  o1recs['p-values'] = pvals
  o1recs=o1recs.sort_values(by='adjcount',ascending=False)
  o1recs=o1recs.head(amount)
  pvals = []
  for x in range(amount):
    pvals.append((firstorder(o1recs.iloc[x,0][0],dfphen, phenotype)))
  o1recs['p-values'] = pvals
  return o1recs

def secondorder(snp1, snp2, dfphen, phenotype):
  '''This function performs a t-test to determine if the second order snp coalition [snp1,snp2] is statistically significant using dfphen. 
  
  Other parameters include: 
  dfphen: The dataframe containing all the individuals and the phenotype. Returned by import_clean(phen, cols, path). 
  phenotype: The phenotype string. Returned by import_clean(phen, cols, path).


  In other words, we determine if there is sufficient evidence that the presence of [snp1,snp2] influences the phenotype. The p-value is returned. '''
  #Group where snp1 and snp2 are on
  true = dfphen.loc[(dfphen[snp1] >= 1) & (dfphen[snp2] >= 1), phenotype]

  #Group where snp1 and snp2 are off
  false = dfphen.loc[(dfphen[snp1] == 0) | (dfphen[snp2] == 0), phenotype]
    
  #p-value
  return stats.ttest_ind(true,false).pvalue

def order2groupings(originaldf, specifieddf, amount, dfphen, phenotype):

  '''As inputs, this function takes the original dataframe of the study containing all of the individuals, the dataframe containing the specified individuals to consider, the amount of second order SNP coalitions to be recommended, the original dataframe containing all individuals and genotypes (called dfphen, which was returned by import_clean(phen, cols, path)), and the phenotype. 
  
  Within this function, the total occurrences of each possible second order snp combination in the specifieddf is counted. Then, an adjusted count is calculated. 
  The adjusted count takes the occurrences of each possible second order snp combination in specifieddf, and divides by the occurrences in the originaldf. 
  The SNP coalitions corresponding to the top adjusted counts are tested using a t-test and a dataframe called o2recs containing the recommended SNP groupings, the count, the adjusted count, and their p-values is returned. 

  Additionally, o2recs, which is the order 2 recommendations is returned. 
  '''
  combs = combinations(list(range(0,len(dfphen.axes[1])-1)),2)
  final2_arr = []

  for i in list(combs):
    final2_arr.append(i)

  #final2_arr is the array of all second order combinations. 
  final2_arr


  second_arr = []
  #goes through each combination
  for i in final2_arr:
    MySum = 0
    #for each of the individuals that were detected
    for j in specifieddf.index: 
      if specifieddf.loc[j,str(i[0])] == 1 and specifieddf.loc[j,str(i[1])] == 1:
        MySum +=1
    second_arr.append(MySum)

  totalsecond_arr = []

  #goes through each combination
  for i in final2_arr:
    MySum = 0
    #for each of the individuals that were detected
    for j in originaldf.index: 
      if originaldf.loc[j,str(i[0])] == 1 and originaldf.loc[j,str(i[1])] == 1:
        MySum +=1
    totalsecond_arr.append(MySum)

  #totalsecond_arr is the number of people that have each grouping out of everyone that was originally in the study (141 individuals. )

  dfAlphMax2 = pd.DataFrame()
  dfAlphMax2['Second Order']  = final2_arr
  dfAlphMax2['Calculations']  = second_arr
  dfAlphMax2['Total Calculations'] = totalsecond_arr


  snps = dfAlphMax2.loc[:,'Second Order']
  o2recs = pd.DataFrame()
  o2recs['snps'] = snps
  o2recs['count']=dfAlphMax2.loc[:,'Calculations']
  o2recs['adjcount'] = dfAlphMax2.loc[:,'Calculations']/dfAlphMax2.loc[:,'Total Calculations']
  o2recs=o2recs.sort_values(by='adjcount',ascending=False)
  o2recs=o2recs.head(amount)
  pvals = []

  for x in o2recs.index:
    pvals.append((secondorder(str(o2recs.loc[x,'snps'][0]),str(o2recs.loc[x,'snps'][1]), dfphen, phenotype)))
  o2recs['p-values'] = pvals
  return o2recs

def thirdorder(snp1, snp2, snp3, dfphen, phenotype):
  '''This function performs a t-test to determine if the third order snp coalition [snp1,snp2,snp3] is statistically significant using dfphen. 
  
  Other parameters include: 
  dfphen: The dataframe containing all the individuals and the phenotype. Returned by import_clean(phen, cols, path). 
  phenotype: The phenotype string. Returned by import_clean(phen, cols, path).


  In other words, we determine if there is sufficient evidence that the presence of [snp1,snp2,snp3] influences the phenotype. The p-value is returned. '''

  #Group where snp1,snp2 and snp3 are on
  true = dfphen.loc[(dfphen[snp1] >= 1) & (dfphen[snp2] >= 1) & (dfphen[snp3] >= 1), phenotype]

  #Group where snp1, snp2, or snp3 are off
  false = dfphen.loc[(dfphen[snp1] == 0) | (dfphen[snp2] == 0) | (dfphen[snp3] == 0), phenotype]
    
  #p-value
  return stats.ttest_ind(true,false).pvalue

def order3groupings(originaldf, specifieddf, amount, dfphen, phenotype):
  '''As inputs, this function takes the original dataframe of the study containing all of the individuals, the dataframe containing the specified individuals to consider, the amount of second order SNP coalitions to be recommended, the original dataframe containing all individuals and genotypes (called dfphen, which was returned by import_clean(phen, cols, path)), and phenotype. 
  
  Within this function, the total occurrences of each possible third order snp combination in the specifieddf is counted. Then, an adjusted count is calculated. 
  The adjusted count takes the occurrences of each possible third order snp combination in specifieddf, and divides by the occurrences in the originaldf. 
  The SNP coalitions corresponding to the top adjusted counts are tested using a t-test and a dataframe called o3recs containing the recommended SNP groupings, the count, the adjusted count, and their p-values is returned. 

  Additionally, o3recs, which is the order 3 recommendations is returned. 
  '''
  combs = combinations(list(range(0,len(dfphen.axes[1])-1)),3)
  final3_arr = []

  for i in list(combs):
    final3_arr.append(i)

  third_arr = []
  #goes through each combination
  for i in final3_arr:
    MySum = 0
    #for each of the individuals that were detected
    for j in specifieddf.index: 
      if specifieddf.loc[j,str(i[0])] == 1 and specifieddf.loc[j,str(i[1])] == 1 and specifieddf.loc[j,str(i[2])] == 1:
        MySum +=1
    third_arr.append(MySum)

  totalthird_arr = []

  #goes through each combination
  for i in final3_arr:
    MySum = 0
    #for each of the individuals that were detected
    for j in originaldf.index: 
      if originaldf.loc[j,str(i[0])] == 1 and originaldf.loc[j,str(i[1])] == 1 and originaldf.loc[j,str(i[2])] == 1:
        MySum +=1
    totalthird_arr.append(MySum)

  #totalthird_arr is the number of people that have each grouping out of everyone that was originally in the study (141 individuals. )

  dfAlphMax2 = pd.DataFrame()
  dfAlphMax2['Third Order']  = final3_arr
  dfAlphMax2['Calculations']  = third_arr
  dfAlphMax2['Total Calculations'] = totalthird_arr

  snps = dfAlphMax2.loc[:,'Third Order']
  o3recs = pd.DataFrame()
  o3recs['snps'] = snps
  o3recs['count']=dfAlphMax2.loc[:,'Calculations']
  o3recs['adjcount'] = dfAlphMax2.loc[:,'Calculations']/dfAlphMax2.loc[:,'Total Calculations']
  o3recs=o3recs.sort_values(by='adjcount',ascending=False)
  o3recs=o3recs.head(amount)
  pvals = []
  for x in o3recs.index:
    pvals.append((thirdorder(str(o3recs.loc[x,'snps'][0]),str(o3recs.loc[x,'snps'][1]),str(o3recs.loc[x,'snps'][2]), dfphen, phenotype)))
  o3recs['p-values'] = pvals
  return o3recs

def fourthorder(snp1, snp2, snp3, snp4, dfphen, phenotype):
  '''This function performs a t-test to determine if the fourth order snp coalition [snp1,snp2,snp3,snp4] is statistically significant using dfphen. 
  
  Other parameters include: 
  dfphen: The dataframe containing all the individuals and the phenotype. Returned by import_clean(phen, cols, path). 
  phenotype: The phenotype string. Returned by import_clean(phen, cols, path).

  In other words, we determine if there is sufficient evidence that the presence of [snp1,snp2,snp3,snp4] influences the phenotype. The p-value is returned. '''
  true = dfphen.loc[(dfphen[snp1] >= 1) & (dfphen[snp2] >= 1) & (dfphen[snp3] >= 1) & (dfphen[snp4] >= 1), phenotype]


  false = dfphen.loc[(dfphen[snp1] == 0) | (dfphen[snp2] == 0) | (dfphen[snp3] == 0) | (dfphen[snp4] == 0), phenotype]
    
  return stats.ttest_ind(true,false).pvalue

def order4groupings(originaldf, specifieddf, amount, dfphen, phenotype):

  '''As inputs, this function takes the original dataframe of the study containing all of the individuals, the dataframe containing the specified individuals to consider, the amount of second order SNP coalitions to be recommended, the original dataframe containing all individuals and genotypes (called dfphen, which was returned by import_clean(phen, cols, path)), and phenotype. 
  
  Within this function, the total occurrences of each possible fourth order snp combination in the specifieddf is counted. Then, an adjusted count is calculated. 
  The adjusted count takes the occurrences of each possible fourth order snp combination in specifieddf, and divides by the occurrences in the originaldf. 
  The SNP coalitions corresponding to the top adjusted counts are tested using a t-test and a dataframe called o4recs containing the recommended SNP groupings, the count, the adjusted count, and their p-values is returned. 

  Additionally, o4recs, which is the order 4 recommendations is returned.
  '''
  combs = combinations(list(range(0,len(dfphen.axes[1])-1)),4)
  final4_arr = []

  for i in list(combs):
    final4_arr.append(i)

  fourth_arr = []
  #goes through each combination
  for i in final4_arr:
    MySum = 0
    #for each of the individuals that were detected
    for j in specifieddf.index: 
      if specifieddf.loc[j,str(i[0])] == 1 and specifieddf.loc[j,str(i[1])] == 1 and specifieddf.loc[j,str(i[2])] == 1 and specifieddf.loc[j,str(i[3])] == 1:
        MySum +=1
    fourth_arr.append(MySum)

  totalfourth_arr = []

  #goes through each combination
  for i in final4_arr:
    MySum = 0
    #for each of the individuals that were detected
    for j in originaldf.index: 
      if originaldf.loc[j,str(i[0])] == 1 and originaldf.loc[j,str(i[1])] == 1 and originaldf.loc[j,str(i[2])] == 1 and originaldf.loc[j,str(i[3])] == 1:
        MySum +=1
    totalfourth_arr.append(MySum)

  #totalfourth_arr is the number of people that have each grouping out of everyone that was originally in the study (141 individuals. )

  dfAlphMax2 = pd.DataFrame()
  dfAlphMax2['Fourth Order']  = final4_arr
  dfAlphMax2['Calculations']  = fourth_arr
  dfAlphMax2['Total Calculations'] = totalfourth_arr

  snps = dfAlphMax2.loc[:,'Fourth Order']
  o4recs = pd.DataFrame()
  o4recs['snps'] = snps
  o4recs['count']=dfAlphMax2.loc[:,'Calculations']
  o4recs['adjcount'] = dfAlphMax2.loc[:,'Calculations']/dfAlphMax2.loc[:,'Total Calculations']
  o4recs=o4recs.sort_values(by='adjcount',ascending=False)
  o4recs=o4recs.head(amount)
  pvals = []
  for x in o4recs.index:
    pvals.append((fourthorder(str(o4recs.loc[x,'snps'][0]),str(o4recs.loc[x,'snps'][1]),str(o4recs.loc[x,'snps'][2]),str(o4recs.loc[x,'snps'][3]), dfphen, phenotype)))
  o4recs['p-values'] = pvals
  return o4recs

def recommend1to4order(originaldf, specifieddf, amountorder1,amountorder2,amountorder3,amountorder4, dfphen, phenotype):
  '''This function returns dataframe named recs that consists of first through fourth order SNP coalitions, their counts, adjusted counts, and pvalues. 
  
  Inputs: originaldf - the oridinal df formed when import_clean() is called. This is the main dataframe containing your individuals you wish to compare your recommended individuals to. (dfphen, or reduceddf after NERDS). 
  specifieddf - this is the dataframe that consists of individuals recommended by your system. 
  amountorder1,...,amountorder4 - the amount of recommendations of SNP groupings wanted of each order. 
  dfphen - the original dataframe consisting of all individuals and phenotypes. 
  phenotype: string of phenotype name 
  '''

  o1recs = order1groupings(originaldf, specifieddf, amountorder1, dfphen, phenotype)
  o2recs = order2groupings(originaldf, specifieddf, amountorder2, dfphen, phenotype)
  o3recs = order3groupings(originaldf, specifieddf, amountorder3, dfphen, phenotype)
  o4recs = order4groupings(originaldf, specifieddf, amountorder4, dfphen, phenotype)
  recs = pd.concat([o1recs, o2recs, o3recs, o4recs])
  return recs

def filter_for_NERDS(inputdf, dfphen, phenotype): 
  '''This function takes a dataframe containing specified individuals called inputdf, dfphen (the original dataframe containing all individuals and genotypes returned by import_clean(phen, cols, path)), and phenotype (the phenotype string returned by import_clean(phen, cols, path)) as parameters. 
  
  Within this function, the phenotype column is saved, and the dataframe is filtered by uniqueness. 
  If there are any individuals with the same genotype, duplicates are removed while averaging the phenotypes. 
  The dataframe, called reduceddf is reorganized and returned as reduceddf, along with its corresponding numpy array reducedarray.'''


  #save the phenotype column
  phencol= inputdf[phenotype]
  originaldf = inputdf
  #drop the phenotype column
  originaldf = originaldf.drop([phenotype], axis = 1)
  intdf = originaldf
  #make all values integers
  intdf = intdf.applymap(int)
  #make all values strings
  strdf = intdf.applymap(str)
  #create unique ids based on genotype
  strdf

  #Get names of columns
  colnames = []
  for x in list(range(0,len(dfphen.axes[1])-1)):
    colnames.append(str(x))

  strdf['id'] = strdf[colnames].sum(axis=1).map(hash)
  #add the phenotype column back
  uniqueid_df=pd.concat([phencol,strdf],axis=1)

  #group by genotype
  g1= uniqueid_df.groupby('id')
  uniqueid_df.groupby('id').size()

  #find the average phenotype for each genotype
  g1ave=g1.mean()

  #turn the averages into an array of values
  g2=g1ave.values
  
  #drop duplicates (by genotype) and add the average values to the individuals
  reduced_df=uniqueid_df.sort_values(by=['id'])
  reduced_df=reduced_df.drop([phenotype], axis = 1)
  reduced_df=reduced_df.drop_duplicates()
  reduced_df=pd.DataFrame(reduced_df)
  reduced_df[phenotype]=g2
  
  #reorganize the dataframe
  reduced_df1=reduced_df.drop(['id'], axis = 1)
  avephen=reduced_df1[phenotype]
  reduced_df1=reduced_df1.drop([phenotype], axis = 1)
  reduced_df1.insert(0, phenotype, avephen)
  reduced_df1=reduced_df1.reset_index()
  reduced_df1=reduced_df1.drop(['index'], axis=1)
  reduced_df1=reduced_df1.applymap(float) 
  

  #making this so the rest of the code works with this df
  reduceddf=reduced_df1
  reducedarray=reduced_df1.values

  return reduceddf, reducedarray

def normalize_data(reduceddf, reducedarray): 
  '''This function uses the reducedarray global variable created within filter_for_NERDS() to normalize the data and do Singular Value Decomposition. 
  
  ---Parameters---
  reduceddf: The filtered and reduced dataframe of individuals returned by filter_for_NERDS(inputdf,dfphen, phenotype).
  reducedarray: reduceddf as a numpy array returned by filter_for_NERDS(inputdf,dfphen, phenotype). 

  The normalized matrix is factored into three matrices called U, S, and V, which are returned in that order.
  S, a global variable, consists of the singular values of the matrix, which are used later. 
  '''
  # normalize the matrix
  # first we subtract the column means; T transposes
  normalized_mat = reducedarray - np.asarray([(np.mean(reducedarray, 1))]).T
  # divide by number of individuals
  normalized_mat = (1/len(reduceddf))*normalized_mat
  
  A = normalized_mat.T 
  U, S, V = np.linalg.svd(A) 
  return U,S,V

def sum_prev(S,x):
  '''This function is used to complement the solve_for_c() function. 
  Inputs are the singular values of the normalized matrix, which are in the array S, and a value x, which is the index you would like to sum to. 

  This function uses recursion to sum the values of an array from index 0 in S squared to index x-1. ''' 
  S_squared = S**2
  total = 0
  #base case: if we just want the first value, or if we want the first and the second. 
  if(x == 1):
      total = S_squared[x-1]
      return(total)
  #inductive step: The total is the sum of the x-1 index, and the sum of all of the previous ones. 
  #notice the function calls itself, so it will do sum_prev(S,x-1), sum_prev(S,x-2), etc. until it reaches base case. 
  else:
      total = S_squared[x-1] + sum_prev(S,x-1)
      return(total)

def solve_for_c(percent, S, reduceddf):
  '''This calculates the amount of singular values in S that we should keep. 
  
  ---Parameters--- 
  percent: A thresholding measure used for choosing the number of singular values to keep. 
  S: The singular values of reduceddf after normalization. Returned by normalize_data(reduceddf, reducedarray). 
  reduceddf: The filtered and reduced dataframe of individuals returned by filter_for_NERDS(inputdf,dfphen, phenotype).


  The parameter of the function is the percentage, or a threshold that determines how many singular values are kept. 
  The sum of a fraction of the singular values is divided by the sum of all singluar values. 
  When this calculation results in a number above the percentage given, then that amount of singular values are used. 
  This function returns c, the number of principal components to keep from our normalized matrix. '''
  
  S_squared = S**2
  sum_total = sum(S_squared)
  #print(sum_total)
  kept = 0
  x = 0
  #once we find a value where the percentage is higher than we want it, we will return the amount of sigmas we keep
  for x in range(len(S)-1):
      #use x + 1 because if x = 0, we want to sum the first 1 value. Then, for example, if x = 13, we want to sum the first 14 vals
      if(sum_prev(S,x + 1)/sum_total <= percent ):
          kept = x + 1
          continue 
      else: 
          #the sigmas we keep is x + 1
          kept = x + 1
          break
  #n is the amount of sigmas we are getting rid of, which correlates to how many columns of S we will disregard. 
  n = len(S)-kept
  c = len(reduceddf) - math.floor(len(reduceddf)/len(S))*n
  return c

def print_similar_people(reducedarray, person_id, top_indexes):
  '''This function is used to print the top N similar individuals provided by a similarity metric. 

  The parameters are as follows: 
  reducedarray -  The array that the individuals are listed in. This was created by the filter_for_NERDS function and is a global variable. 
  person_id - the index of the individual that we are comparing to within each similarity metric. 
  top_indexes - this is an array of N indexes consisting of the individuals who are the most similar to person_id. 
   '''
  print(f'Recommendations for {reducedarray[person_id]} : \n')
  for id in top_indexes:
      print(reducedarray[id,:])

def similarity_setup(str_max_or_min, reduceddf, phenotype, V, c):
  '''This function finds the indidivual with the most extreme phenotype, and saves the index called person_id. 
  If the dataset is being analyzed for increases in phenotype, put "max" as the parameter, else put "min". 
  Note that the default is max if the string does not match "max" or "min".

  ---Parameters--- 
  str_max_or_min: "max" or "min" depending on if you are analyzing the dataset for increases or decreases in phenotype. 
  reduceddf: The filtered and reduced dataframe of individuals returned by filter_for_NERDS(inputdf,dfphen, phenotype).
  phenotype: The phenotype string. Returned by import_clean(phen, cols, path).
  V: The matrix that we will slice using SVD. Returned by normalize_data(reduceddf, reducedarray). 
  c: The number of principal components chosen to keep in the function solve_for_c(percent, S, reduceddf). 


  The data is reduced using Principal Component Analysis with the top c components. 
  This reduced matrix is called sliced. 
  This matrix is also normalized and stored as normalsliced. 
  
  The function returns person_id, sliced, and normalsliced'''
  if str_max_or_min == 'min': 
    person_id = reduceddf.nsmallest(1, phenotype).index[0] #person you are comparing similarity with against all reduceddf
  else:
    person_id = reduceddf.nlargest(1, phenotype).index[0]

  sliced = V.T[:, :c] #representative data using top c principal components
  
  normalsliced = (sliced - np.min(sliced)) / (np.max(sliced) - np.min(sliced))

  return person_id, sliced, normalsliced

def top_cosine_similarity(sliced, person_id,c):
  '''This function calculates the cosine similarity of each individual compared to the specified individual person_id. 
  The sliced parameter is a  dataframe created in the similarity_setup() function, but can be replaced by any dataframe that you wish to use cosine similarity on. 
  The paramter c is the number of principal components chosen to keep in the function solve_for_c(percent, S, reduceddf). 
  
  The results are sorted by who is the most similar, and the top c results are called cosindex. 
  Recall that c is corresponds to the number of principal components kept. 
  
  This function returns cosindex.'''

  # define individual of interest
  person = sliced[person_id,:]
  # calculate denominator
  denom = np.linalg.norm(person) * np.linalg.norm(sliced)
  # calculate similarity
  similarity = np.dot(person, sliced.T) / denom
  # sort indices

  cosindex = np.argsort(-similarity)
  cosindex =  cosindex[:c]
  return cosindex

def Jaccard(normalsliced,person_id,c):
  '''This function calculates the jaccard similarity of each individual compared to the specified individual person_id. 
  The normalsliced parameter is a global dataframe created in the similarity_setup() function, but can be replaced by any normalized dataframe that you wish to use jaccard similarity on. 
  The results are sorted by which individuals are the most similar, and the top c results are stored in jacindex, which is returned. 
  Recall that c is a corresponds to the number of principal components kept, and was returned by the function solve_for_c(percent, S, reduceddf).  '''
  df=pd.DataFrame(normalsliced)
  #create jaccard column
  df['Jaccard']=np.zeros(len(df))
  for i in range(len(df)):
  #create blank arrays
      top=[]
      bottom=[] 
      #for each gene
      for j in range(c):
          #compare to individual 63 and put the smaller number in top
          top.append(min(df.iloc[i,j],df.iloc[person_id,j]))
          #compare to individual 63 and put the larger number in bottom
          bottom.append(max(df.iloc[i,j],df.iloc[person_id,j]))
          #sum top/bottom and place in "Jaccard" column
          df.iloc[i,c]=sum(top)/sum(bottom)
  jacindex=df.nlargest(c,'Jaccard').index
  return jacindex

def metric_overlap(cosindex,jacindex, reduceddf, images_dir): 
  '''The first two parameters of this function are cosindex, and jacindex. 
  These correlate to the indexes of the dataframe used in NERDS (reduceddf) that are most similar to person_id based on cosine and jaccard similarity respectively. 
  
  --- Other Parameters --- 
  reduceddf: The filtered and reduced dataframe of individuals returned by filter_for_NERDS(inputdf,dfphen, phenotype).
  images_dir: The path to the directory where your visualization will be saved. This is returned by drive_setup(). 
  
  The individuals that were detected twice are listed in a dataframe called duped, which is returned, and contains our most recommended individuals. 
  Additionally, and a venn diagram is created and stored at the specified file path from drive_setup. 
  
  Following the implementation of this function, recommend1to4order(originaldf, duped, amountorder1, amountorder2, amountorder3, amountorder4) should be called where originaldf is reduceddf or dfphen. 
  This depends on if you want to perform the adjusted count on the whole dataset, or the clustered (reduced) dataset. '''
  allindex = np.concatenate((cosindex, jacindex), axis=None)
  #identify all individuals that are detected more than once

  duped=([item for item, count in collections.Counter(allindex).items() if count > 1])
  duped = reduceddf.loc[duped,:]

  set1 = set(cosindex)
  set2 = set(jacindex)

  venn2([set1, set2], ('Cosine', 'Jaccard'))
  plt.savefig(f"{images_dir}/overlapvenn.png",bbox_inches='tight')
  plt.show()
  return duped
